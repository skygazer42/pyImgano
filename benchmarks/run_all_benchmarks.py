"""
Run all PyImgAno benchmarks.

This script runs all benchmark suites and generates a comprehensive report.
"""

import os
import sys
import subprocess
import time
from datetime import datetime

# Add parent directory to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))


def run_benchmark(script_name: str, description: str) -> tuple:
    """Run a single benchmark script."""
    print("\n" + "="*80)
    print(f"Running: {description}")
    print("="*80)

    start_time = time.time()

    try:
        result = subprocess.run(
            [sys.executable, script_name],
            cwd=os.path.dirname(__file__),
            capture_output=True,
            text=True,
            timeout=600  # 10 minute timeout
        )

        elapsed = time.time() - start_time

        if result.returncode == 0:
            print(result.stdout)
            print(f"\n‚úÖ {description} completed in {elapsed:.1f}s")
            return True, elapsed, None
        else:
            print(f"\n‚ùå {description} failed:")
            print(result.stderr)
            return False, elapsed, result.stderr

    except subprocess.TimeoutExpired:
        elapsed = time.time() - start_time
        error_msg = f"Timeout after {elapsed:.1f}s"
        print(f"\n‚è±Ô∏è {description} timed out")
        return False, elapsed, error_msg

    except Exception as e:
        elapsed = time.time() - start_time
        error_msg = str(e)
        print(f"\n‚ùå {description} error: {error_msg}")
        return False, elapsed, error_msg


def generate_report(results: dict, output_path: str = "benchmark_report.md"):
    """Generate a markdown report of all benchmark results."""

    with open(output_path, 'w') as f:
        f.write("# PyImgAno Benchmark Report\n\n")
        f.write(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")

        f.write("## Summary\n\n")
        total_benchmarks = len(results)
        successful = sum(1 for success, _, _ in results.values() if success)
        failed = total_benchmarks - successful
        total_time = sum(elapsed for _, elapsed, _ in results.values())

        f.write(f"- **Total Benchmarks**: {total_benchmarks}\n")
        f.write(f"- **Successful**: {successful} ‚úÖ\n")
        f.write(f"- **Failed**: {failed} ‚ùå\n")
        f.write(f"- **Total Time**: {total_time:.1f}s ({total_time/60:.1f} minutes)\n\n")

        f.write("## Detailed Results\n\n")

        for name, (success, elapsed, error) in results.items():
            status = "‚úÖ Success" if success else "‚ùå Failed"
            f.write(f"### {name}\n\n")
            f.write(f"- **Status**: {status}\n")
            f.write(f"- **Time**: {elapsed:.2f}s\n")

            if error:
                f.write(f"- **Error**: \n```\n{error}\n```\n")

            f.write("\n")

        f.write("## Generated Files\n\n")
        f.write("The following files should be generated by the benchmarks:\n\n")
        f.write("### Classical Algorithms\n")
        f.write("- `benchmark_classical_results.csv` - Raw data\n")
        f.write("- `benchmark_classical_results.png` - Visualization\n\n")

        f.write("### Deep Learning Algorithms\n")
        f.write("- `benchmark_deeplearning_results.csv` - Raw data\n")
        f.write("- `benchmark_deeplearning_results.png` - Visualization\n\n")

        f.write("### Preprocessing & Augmentation\n")
        f.write("- `benchmark_preprocessing_results.csv` - Raw data\n")
        f.write("- `benchmark_preprocessing_results.png` - Visualization\n\n")

        f.write("## Interpretation\n\n")
        f.write("### Classical Algorithms\n")
        f.write("- **Fast Training**: Statistical methods (IQR, MAD) train instantly\n")
        f.write("- **Fast Inference**: KNN and LOF provide quick predictions\n")
        f.write("- **Low Memory**: Most classical methods have minimal overhead\n")
        f.write("- **Good Accuracy**: Ensemble methods (IForest) and density methods (ECOD, COPOD) "
                "typically achieve best AUC-ROC\n\n")

        f.write("### Deep Learning Algorithms\n")
        f.write("- **Training Time**: Neural networks require more time but learn complex patterns\n")
        f.write("- **Inference Speed**: Once trained, inference is relatively fast\n")
        f.write("- **Model Size**: Modern architectures are optimized for efficiency\n")
        f.write("- **Accuracy**: Deep methods excel on complex visual anomalies\n\n")

        f.write("### Preprocessing Operations\n")
        f.write("- **Basic Ops**: Edge detection, filtering are very fast (<10ms)\n")
        f.write("- **Advanced Ops**: FFT, Retinex, HOG take longer but provide richer features\n")
        f.write("- **Augmentation**: Pipeline speed depends on complexity and number of operations\n")
        f.write("- **Scalability**: Processing time scales with image size\n\n")

        f.write("## Recommendations\n\n")
        f.write("### For Real-time Applications (< 100ms)\n")
        f.write("- Use classical methods: KNN, LOF, or IForest\n")
        f.write("- Apply basic preprocessing only\n")
        f.write("- Use light augmentation during training\n\n")

        f.write("### For High Accuracy (offline processing OK)\n")
        f.write("- Use deep learning: Autoencoder, VAE, or Deep SVDD\n")
        f.write("- Apply advanced preprocessing: Retinex, texture features\n")
        f.write("- Use heavy augmentation for better generalization\n\n")

        f.write("### For Balanced Performance\n")
        f.write("- Use ensemble classical methods: IForest with multiple estimators\n")
        f.write("- Apply medium preprocessing: edge detection + normalization\n")
        f.write("- Use medium augmentation pipeline\n\n")

    print(f"\nReport saved to {output_path}")


def main():
    """Run all benchmarks and generate report."""
    print("\n" + "="*80)
    print("PyImgAno Comprehensive Benchmark Suite")
    print("="*80)
    print(f"\nStarted at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

    # Change to benchmarks directory
    os.chdir(os.path.dirname(__file__))

    results = {}

    # Run preprocessing benchmark
    success, elapsed, error = run_benchmark(
        "benchmark_preprocessing.py",
        "Preprocessing & Augmentation Benchmark"
    )
    results["Preprocessing & Augmentation"] = (success, elapsed, error)

    # Run classical algorithms benchmark
    success, elapsed, error = run_benchmark(
        "benchmark_classical.py",
        "Classical Algorithms Benchmark"
    )
    results["Classical Algorithms"] = (success, elapsed, error)

    # Run deep learning benchmark (may take longer)
    success, elapsed, error = run_benchmark(
        "benchmark_deeplearning.py",
        "Deep Learning Algorithms Benchmark"
    )
    results["Deep Learning Algorithms"] = (success, elapsed, error)

    # Generate comprehensive report
    print("\n" + "="*80)
    print("Generating Benchmark Report")
    print("="*80)
    generate_report(results)

    # Final summary
    print("\n" + "="*80)
    print("All Benchmarks Complete!")
    print("="*80)

    successful = sum(1 for success, _, _ in results.values() if success)
    total = len(results)
    total_time = sum(elapsed for _, elapsed, _ in results.values())

    print(f"\nüìä Results: {successful}/{total} benchmarks successful")
    print(f"‚è±Ô∏è  Total time: {total_time:.1f}s ({total_time/60:.1f} minutes)")
    print(f"\nüìÑ Check benchmark_report.md for detailed results")
    print(f"üìà Check individual CSV and PNG files for detailed metrics\n")


if __name__ == "__main__":
    main()
